{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nih9o1mooEx8",
        "outputId": "71e3b748-94b2-4080-8d5d-d3a49470f814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import contextlib\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "import numpy as np\n",
        "import spacy\n",
        "import string\n",
        "import json\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
        "POS = (\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\")\n",
        "\n",
        "def lemmatize(word):\n",
        "    token = nlp(str(word))[0]\n",
        "    lemma = token.lemma_\n",
        "    inflections = {token._.inflect(pos) for pos in POS}\n",
        "    return lemma, inflections\n",
        "\n",
        "def tokenize(sentence):\n",
        "    tokenized = nlp(sentence.translate(str.maketrans('', '', string.punctuation)))\n",
        "    return [token.lemma_ for token in tokenized if token.lemma_ not in en_stopwords and wordnet.synsets(token.lemma_)]\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "en_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "data = []\n",
        "for i in range (1, 1731):\n",
        "    with contextlib.suppress(FileNotFoundError):\n",
        "        with open(f'data/fairy_tales/{i}.txt', 'rb') as f:\n",
        "            data.append(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "####################### MOVE THE TRAINING TO GPU USING .DEVICE #######################\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJN3GM3qoWLc",
        "outputId": "f1236624-6c9a-4870-ce51-3e80188d95b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTqpXio1oEyB"
      },
      "outputs": [],
      "source": [
        "# decoded_data = []\n",
        "# for i, row in enumerate(data):\n",
        "#     new_doc = \"\"\n",
        "\n",
        "#     try:\n",
        "#         sentences = sent_tokenize(row.decode('cp1252').lower())\n",
        "#     except UnicodeDecodeError:\n",
        "#         sentences = sent_tokenize(row.decode('utf8').lower())\n",
        "#     for sentence in sentences:\n",
        "#         tokenized = tokenize(sentence)\n",
        "#         new_doc += \" \" + \" \".join(tokenized)\n",
        "\n",
        "#     decoded_data.append(new_doc)\n",
        "#     print(f\"{i}th row processed\")\n",
        "\n",
        "# with open('data/fairytales_tokenized_lemmatized.json', 'w') as f:\n",
        "#     json.dump(decoded_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1hb3zQW7oEyC"
      },
      "outputs": [],
      "source": [
        "with open('fairytales_tokenized_lemmatized.json', 'r') as f:\n",
        "    decoded_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dhS-6P3boEyD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "word_count_vector = cv.fit_transform(decoded_data)\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_transformer.fit(word_count_vector)\n",
        "\n",
        "idfs = pd.DataFrame(tfidf_transformer.idf_, index=feature_names, columns=[\"idf_weights\"]).sort_values(by=['idf_weights'])\n",
        "\n",
        "count_vector = cv.transform(decoded_data)\n",
        "tf_idf_vector = np.asarray(tfidf_transformer.transform(count_vector).todense())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute TF-IDFs and bloom filters for each word"
      ],
      "metadata": {
        "id": "nojAsljipeQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mmh3\n",
        "import mmh3\n",
        "\n",
        "def hash_digests(token, bits):\n",
        "    return [mmh3.hash(token, i) % bits for i in range(3)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JNKo9NEpd9i",
        "outputId": "ec0a6b63-2a19-4014-de2d-e660855df5b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mmh3\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mmh3\n",
            "Successfully installed mmh3-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bUKa0A3-oEyE"
      },
      "outputs": [],
      "source": [
        "json_tf_idfs = {f'doc{i}': dict(sorted({feature_names[j]: tf_idf_vector[i][j] for j in range(18249)\n",
        "                                        if tf_idf_vector[i][j] > 0}.items(),\n",
        "                                        key=lambda item: item[1], reverse=True))\n",
        "                for i in range(len(decoded_data))}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_tf_idfs_bloom_filters = {}\n",
        "\n",
        "for i in range(len(decoded_data)):\n",
        "    doc_key = f'doc{i}'       # document key --> ex: doc0, doc1\n",
        "    tf_idf_bloom_filter = {}  # dictionary to hold tf-idf and bloom filter\n",
        "    for word, tf_idf in json_tf_idfs[doc_key].items():\n",
        "        tf_idf_bloom_filter[word] = {\n",
        "            'tf_idf': tf_idf,\n",
        "            'bloom_filter': hash_digests(word, 32)  # size of bloom filter=32\n",
        "        }\n",
        "    json_tf_idfs_bloom_filters[doc_key] = tf_idf_bloom_filter\n",
        "\n",
        "print(\"Done computing tf-idfs and bloom filters in dictionary...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgyFa3QwtGuy",
        "outputId": "024f88c6-ffde-43d2-c1e2-233d0e793ebe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done computing tf-idfs and bloom filters in dictionary...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w7iUyOKooEyF"
      },
      "outputs": [],
      "source": [
        "with open('fairytales_doc_tf-idf_bloom_filters.json', 'w') as f:\n",
        "    json.dump(json_tf_idfs_bloom_filters, f, indent=4)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}