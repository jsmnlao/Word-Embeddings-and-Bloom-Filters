{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import mmh3\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_token(token, bits):\n",
    "    representation = np.zeros(bits)\n",
    "    \n",
    "    for i in range(3):\n",
    "        digest = mmh3.hash(token, i) % bits\n",
    "        representation[digest] = 1\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fairytales_word_tf-idfs.json', 'r') as f:\n",
    "    tf_idfs = json.load(f)\n",
    "with open('data/fairytales_word_bloom-filters.json', 'r') as f:\n",
    "    bloom_filters = json.load(f)\n",
    "with open('data/fairytales_tokenized.json', 'r') as f:\n",
    "    tokenized_corpus = json.load(f)\n",
    "with open('data/iterative_vectors/9.json', 'r') as f:\n",
    "    iterative_vectors = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_from_bloom(word, tokenized_sentence, bits, deltas):\n",
    "    representations = []\n",
    "    instance_representation = np.zeros(bits)\n",
    "    indices = [i for i, x in enumerate(tokenized_sentence) if x == word]\n",
    "    \n",
    "    for index in indices:\n",
    "        adjacent_words = 0\n",
    "        for delta in deltas:\n",
    "            if index + delta < 0:\n",
    "                continue\n",
    "            with contextlib.suppress(IndexError):\n",
    "                adjacent_word = tokenized_sentence[index + delta]\n",
    "                try:\n",
    "                    tf_idf = tf_idfs[word][adjacent_word]\n",
    "                except KeyError:\n",
    "                    tf_idf = 0\n",
    "                instance_representation += np.array(bloom_filters[adjacent_word]) * tf_idf\n",
    "                adjacent_words += 1\n",
    "        representations.append((instance_representation/adjacent_words).tolist())\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_from_bloom_no_tfidfs(word, tokenized_sentence, bits, deltas):\n",
    "    representations = []\n",
    "    instance_representation = np.zeros(bits)\n",
    "    indices = [i for i, x in enumerate(tokenized_sentence) if x == word]\n",
    "    \n",
    "    for index in indices:\n",
    "        adjacent_words = 0\n",
    "        for delta in deltas:\n",
    "            if index + delta < 0:\n",
    "                continue\n",
    "            with contextlib.suppress(IndexError):\n",
    "                adjacent_word = tokenized_sentence[index + delta]\n",
    "                instance_representation += np.array(bloom_filters[adjacent_word])\n",
    "                adjacent_words += 1\n",
    "        representations.append((instance_representation/adjacent_words).tolist())\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_from_iterative_vectors(word, tokenized_sentence, bits, deltas):\n",
    "    representations = []\n",
    "    instance_representation = np.zeros(bits)\n",
    "    indices = [i for i, x in enumerate(tokenized_sentence) if x == word]\n",
    "    \n",
    "    for index in indices:\n",
    "        adjacent_words = 0\n",
    "        for delta in deltas:\n",
    "            if index + delta < 0:\n",
    "                continue\n",
    "            with contextlib.suppress(IndexError):\n",
    "                adjacent_word = tokenized_sentence[index + delta]\n",
    "                try:\n",
    "                    tf_idf = tf_idfs[word][adjacent_word]\n",
    "                except KeyError:\n",
    "                    tf_idf = 0\n",
    "                instance_representation += np.array(iterative_vectors[adjacent_word]) * tf_idf\n",
    "                adjacent_words += 1\n",
    "        representations.append((instance_representation/adjacent_words).tolist())\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vectors(word, generation_function, deltas=None, bits=32):\n",
    "    if deltas is None:\n",
    "        deltas = [-4, -3, -2, -1, 1, 2, 3, 4]\n",
    "\n",
    "    representations = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        if word in sentence:\n",
    "            representations += generation_function(word, sentence, bits, deltas)\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_encoding(word, fname, args):\n",
    "    vector = extract_vectors(word, **args)\n",
    "    \n",
    "    with open(fname, 'r') as f:\n",
    "        vectors = json.load(f)\n",
    "    vectors[word] = vector\n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(vectors, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['king', 'queen', 'man', 'woman', 'long', 'man', 'say', 'water']\n",
    "for word in words:\n",
    "    store_encoding(word, 'data/indiv_word_representations/generate_vector_from_bloom.json', \n",
    "                   {'deltas': [-4, -3, -2, -1, 1, 2, 3, 4], 'bits':32, 'generation_function':generate_vector_from_bloom})\n",
    "    store_encoding(word, 'data/indiv_word_representations/generate_vector_from_bloom_no_tfidfs.json', \n",
    "                   {'deltas': [-4, -3, -2, -1, 1, 2, 3, 4], 'bits':32, 'generation_function':generate_vector_from_bloom_no_tfidfs})\n",
    "    store_encoding(word, 'data/indiv_word_representations/generate_vector_from_iterative_vectors.json', \n",
    "                   {'deltas': [-4, -3, -2, -1, 1, 2, 3, 4], 'bits':32, 'generation_function':generate_vector_from_iterative_vectors})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
