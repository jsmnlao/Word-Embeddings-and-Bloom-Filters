{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Yourui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Yourui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import contextlib\n",
    "import string\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "import lemminflect\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "POS = (\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\")\n",
    "\n",
    "def lemmatize(word): # Takes a word and uses the spacy lemmatizer to return the lemmatized form\n",
    "    token = nlp(str(word))[0]\n",
    "    lemma = token.lemma_\n",
    "    inflections = {token._.inflect(pos) for pos in POS} # returns the inflection of the lemmatized token. (ex: run -> {'ran', 'run', 'runner', 'runnest', 'running', 'runs'} )\n",
    "    return lemma, inflections\n",
    "\n",
    "def tokenize(sentence): # Tokenizes a sentence and lemmatizes the words within\n",
    "    tokenized = nlp(sentence.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return [token.lemma_ for token in tokenized if token.lemma_.lower() not in en_stopwords and wordnet.synsets(token.lemma_)] # disregards lemmatized token if it's in list of stopwords or not in english dictionary (wordnet)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "data = [] # loads the datasets as binaries \n",
    "for i in range (1, 1731):\n",
    "    with contextlib.suppress(FileNotFoundError):\n",
    "        with open(f'data/fairy_tales/{i}.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_frequencies = {}\n",
    "total_frequencies = {}\n",
    "tokenized_corpus = []\n",
    "deltas = [-4, -3, -2, -1, 1, 2, 3, 4]\n",
    "words_in_corpus = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_total_frequencies(word): # updates the freq of word in the entire corpus\n",
    "    try:\n",
    "        total_frequencies[word] += 1\n",
    "    except KeyError:\n",
    "        total_frequencies[word] = 1 # if not in list already, add to list and set freq to 1\n",
    "\n",
    "def update_neighbor_frequencies(i, tokenized): # given a tokenized sentence, updates the freq of words next to the word at index i\n",
    "    if tokenized[i] not in neighbor_frequencies.keys(): # creates a dictionary for word at index i if it doesn't already exist\n",
    "        neighbor_frequencies[tokenized[i]] = {}\n",
    "    for delta in deltas: # for the 4 words before and after index i\n",
    "        if i + delta < 0: # skip wrap-arounds\n",
    "            continue\n",
    "        with contextlib.suppress(IndexError): # if a word at the 4 indices before and after i doesn't exist, just ignore\n",
    "            try:\n",
    "                neighbor_frequencies[tokenized[i]][tokenized[i+delta]] += 1\n",
    "            except KeyError:\n",
    "                neighbor_frequencies[tokenized[i]][tokenized[i+delta]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(data):\n",
    "    try: \n",
    "        sentences = sent_tokenize(row.lower()) # converts to lowercase, sent_tokenize tokenizes the document into individual sentences by punctuation\n",
    "    except UnicodeDecodeError:\n",
    "        sentences = sent_tokenize(row.lower())\n",
    "    \n",
    "    for sentence in sentences: # for each sentence in the document\n",
    "        tokenized = tokenize(sentence)\n",
    "        if len(tokenized) > 1: # drops 1-word sentences\n",
    "            tokenized_corpus.append(tokenized)\n",
    "            words_in_corpus += len(tokenized)\n",
    "            for token_index in range(len(tokenized)): # for each word (ex: 'trick' (see the first printed output)) in tokenized sentence, update the freq of the adjacent words ('long', 'ramble', 'belgium', 'storyteller', 'find' for that word)\n",
    "                update_total_frequencies(tokenized[token_index]) # updates the freq of the word at that index in the entire corpus (here, 'trick')\n",
    "                update_neighbor_frequencies(token_index, tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = {word:sum(neighbor_frequencies[word].values()) for word in neighbor_frequencies.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "tf_idfs = copy.deepcopy(neighbor_frequencies) # make copy of the relative frequencies json to convert frequencies to tf-idfs\n",
    "for word in neighbor_frequencies.keys(): # ex: \"queen\"\n",
    "    for neighbor in neighbor_frequencies[word].keys(): # The words next to \"queen\", i.e. \"lady\", \"fairy\", \"king\", etc.\n",
    "        tf_idfs[word][neighbor] = neighbor_frequencies[word][neighbor] / n_neighbors[word]  * math.log(words_in_corpus / n_neighbors[neighbor])\n",
    "\n",
    "for word in tf_idfs.keys():\n",
    "    tf_idfs[word] = dict(sorted(tf_idfs[word].items(), key=lambda item: item[1], reverse=True)) # sorts TF-IDFs in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fairytales_word_tf-idfs.json', 'w') as f:\n",
    "    json.dump(tf_idfs, f, indent=4)\n",
    "with open('data/fairytales_tokenized.json', 'w') as f:\n",
    "    json.dump(tokenized_corpus, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
